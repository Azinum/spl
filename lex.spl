// lex.spl

// enum Token_type {
const T_EOF 0;

const T_IDENTIFIER 1;
const T_NUMBER 2;
const T_CSTRING 3;
const T_ASSIGN 4;
const T_COMMA 5;
const T_AT 6;
const T_DEREF 7;
const T_ADD 8;
const T_SUB 9;
const T_MUL 10;
const T_DIV 11;
const T_DIVMOD 12;
const T_LSHIFT 13;
const T_RSHIFT 14;
const T_LT 15;
const T_GT 16;
const T_AND 17;
const T_LOGICAL_NOT 18;
const T_OR 19;
const T_EQ 20;
const T_NEQ 21;
const T_SEMICOLON 22;
const T_POP 23;
const T_CONST 24;
const T_LET 25;
const T_MEMORY 26;
const T_PRINT 27;
const T_INCLUDE 28;
const T_FN 29;
const T_ARROW 30;
const T_WHILE 31;
const T_IF 32;
const T_ELSE 33;
const T_LEFT_P 34;
const T_RIGHT_P 35;
const T_LEFT_BRACKET 36;
const T_RIGHT_BRACKET 37;
const T_LEFT_CURLY 38;
const T_RIGHT_CURLY 39;
const T_STORE64 40;
const T_STORE32 41;
const T_STORE16 42;
const T_STORE8 43;
const T_LOAD64 44;
const T_LOAD32 45;
const T_LOAD16 46;
const T_LOAD8 47;
const T_SIZEOF 48;

// built-in types
const T_NONE 49;
const T_ANY 50;
const T_UNSIGNED64 51;
const T_CSTR 52;

const MAX_TOKEN_TYPE 53;
// };

let token_type_str "";

let token.buffer 0;
let token.length 0;
let token.type 0;
let token.v 0;
let token.filename "";
let token.source "";
let token.line 0;
let token.column 0;

// token struct offsets
const Token.buffer 0;
const Token.length sizeof u64;
const Token.type + Token.length sizeof u64;
const Token.v + Token.type sizeof u64;
const Token.filename + Token.v sizeof cstr;
const Token.source + Token.filename sizeof cstr;
const Token.line + Token.source sizeof u64;
const Token.column + Token.line sizeof u64;
const Token.size + Token.column sizeof u64;

// struct Lexer {
let l.filename "";
let l.source "";
let l.index 0;
let l.line 0;
let l.column 0;
let l.status 0;
// };

fn token_init(any token, any buffer, u64 length, u64 type, any filename, any source) -> none {
  = + token Token.buffer   buffer;
  = + token Token.length   length;
  = + token Token.type     type;
  = + token Token.v        0;
  = + token Token.filename filename;
  = + token Token.source   source;
  = + token Token.line     1;
  = + token Token.column   1;
}

fn token_print(any token) -> none {
  let tmp tmp_it;
  let p tmp_push_cstr;

  let buffer   # + token Token.buffer;
  let length   # + token Token.length;
  let type     # + token Token.type;
  let value    # + token Token.v;
  let filename # + token Token.filename;
  let source   # + token Token.source;
  let line     # + token Token.line;
  let column   # + token Token.column;

  if eq filename 0 = @filename "none";

  p("buffer   = "); tmp_push_str(buffer, length);
  p("\n");
  p("length   = "); tmp_push_u64(length);
  p("\n");
  p("type     = "); tmp_push_u64(type);
  p("\n");
  p("filename = "); p(filename);
  p("\n");
  p("line     = "); tmp_push_u64(line);
  p("\n");
  p("column   = "); tmp_push_u64(column);
  p("\n");

  puts(STDOUT_FILENO, tmp);
  = @tmp_it tmp;
}

fn is_digit(u64 ch) -> u64 {
  and > ch - '0' 1 < ch + '9' 1;
}

fn is_hex(u64 ch) -> u64 {
  or or is_digit(ch) (and > ch - 'a' 1 < ch + 'f' 1) (and > ch - 'A' 1 < ch + 'F' 1);
}

fn is_alpha(u64 ch) -> u64 {
  or (and > ch - 'a' 1 < ch + 'z' 1) (and > ch - 'A' 1 < ch + 'Z' 1);
}

fn is_extended_ascii(u64 ch) -> u64 {
  and > ch 127 < ch 255;
}

fn to_lower(u64 ch) -> u64 {
  if and > ch - 'A' 1 < ch + 'Z' 1 {
    = @ch + 32 ch;
  }
  ch;
}

fn lexer_read_symbol -> none {
  while or or or or or
    is_alpha(load8 l.index)
    is_digit(load8 l.index)
    is_extended_ascii(load8 l.index)
    eq load8 l.index '_'
    eq load8 l.index '-'
    eq load8 l.index '.' {
    = @l.index + 1 l.index;
    = @l.column + 1 l.column;
  }
  = @token.length - l.index token.buffer;

  if eq 0 strncmp(token.buffer, "pop", token.length) {
    = @token.type T_POP;
  }
  else if eq 0 strncmp(token.buffer, "print", token.length) {
    = @token.type T_PRINT;
  }
  else if eq 0 strncmp(token.buffer, "include", token.length) {
    = @token.type T_INCLUDE;
  }
  else if eq 0 strncmp(token.buffer, "const", token.length) {
    = @token.type T_CONST;
  }
  else if eq 0 strncmp(token.buffer, "let", token.length) {
    = @token.type T_LET;
  }
  else if eq 0 strncmp(token.buffer, "memory", token.length) {
    = @token.type T_MEMORY;
  }
  else if eq 0 strncmp(token.buffer, "fn", token.length) {
    = @token.type T_FN;
  }
  else if eq 0 strncmp(token.buffer, "while", token.length) {
    = @token.type T_WHILE;
  }
  else if eq 0 strncmp(token.buffer, "if", token.length) {
    = @token.type T_IF;
  }
  else if eq 0 strncmp(token.buffer, "else", token.length) {
    = @token.type T_ELSE;
  }
  else if eq 0 strncmp(token.buffer, "store64", token.length) {
    = @token.type T_STORE64;
  }
  else if eq 0 strncmp(token.buffer, "store32", token.length) {
    = @token.type T_STORE32;
  }
  else if eq 0 strncmp(token.buffer, "store16", token.length) {
    = @token.type T_STORE16;
  }
  else if eq 0 strncmp(token.buffer, "store8", token.length) {
    = @token.type T_STORE8;
  }
  else if eq 0 strncmp(token.buffer, "load64", token.length) {
    = @token.type T_LOAD64;
  }
  else if eq 0 strncmp(token.buffer, "load32", token.length) {
    = @token.type T_LOAD32;
  }
  else if eq 0 strncmp(token.buffer, "load16", token.length) {
    = @token.type T_LOAD16;
  }
  else if eq 0 strncmp(token.buffer, "load8", token.length) {
    = @token.type T_LOAD8;
  }
  else if eq 0 strncmp(token.buffer, "and", token.length) {
    = @token.type T_AND;
  }
  else if eq 0 strncmp(token.buffer, "not", token.length) {
    = @token.type T_LOGICAL_NOT;
  }
  else if eq 0 strncmp(token.buffer, "or", token.length) {
    = @token.type T_OR;
  }
  else if eq 0 strncmp(token.buffer, "eq", token.length) {
    = @token.type T_EQ;
  }
  else if eq 0 strncmp(token.buffer, "neq", token.length) {
    = @token.type T_NEQ;
  }
  else if eq 0 strncmp(token.buffer, "lshift", token.length) {
    = @token.type T_LSHIFT;
  }
  else if eq 0 strncmp(token.buffer, "rshift", token.length) {
    = @token.type T_RSHIFT;
  }
  else if eq 0 strncmp(token.buffer, "sizeof", token.length) {
    = @token.type T_SIZEOF;
  }
  else if eq 0 strncmp(token.buffer, "none", token.length) {
    = @token.type T_NONE;
  }
  else if eq 0 strncmp(token.buffer, "any", token.length) {
    = @token.type T_ANY;
  }
  else if eq 0 strncmp(token.buffer, "u64", token.length) {
    = @token.type T_UNSIGNED64;
  }
  else if eq 0 strncmp(token.buffer, "cstr", token.length) {
    = @token.type T_CSTR;
  }
  else {
    = @token.type T_IDENTIFIER;
  }
}

fn lexer_read_number -> none {
  while or is_hex(load8 l.index) eq load8 l.index 'x' {
    = @l.index + 1 l.index;
    = @l.column + 1 l.column;
  }
  = @token.length - l.index token.buffer;
  = @token.type T_NUMBER;
}

fn lexer_error(cstr message) -> none {
  if eq l.status NoError {
    let it tmp_it;
    // a bit noisey, but this will do for now
    // TODO: varadic functions to the rescue
    tmp_push_cstr("[lex-error]: ");
    tmp_push_cstr(token.filename);
    tmp_push_cstr(":");
    tmp_push_u64(token.line);
    tmp_push_cstr(":");
    tmp_push_u64(token.column);
    tmp_push_cstr(": ");
    tmp_push_cstr(message);
    puts(STDERR_FILENO, it);
    = @tmp_it it;
    = @l.status Error;
  }
}

fn next -> none {
  = @token.buffer l.index;
  = @token.length 1;
  = @token.line l.line;
  = @token.column l.column;
}

fn lexer_next -> none {
  let done 0;
  let ch 0;
  while not done {
    next();
    = @ch load8 token.buffer;
    = @token.filename l.filename;
    = @token.source l.source;
    = @token.column l.column;
    = @l.index + 1 l.index;
    = @l.column + 1 l.column;

    if eq ch 13 { // `\r`
      = @l.column 1;
      if neq 10 load8 l.index {
        = @l.line + 1 l.line
      }
      = @token.column l.column;
    }
    else if eq ch 10 { // `\n`
      = @l.line + 1 l.line;
      = @l.column 1;
      = @token.column l.column;
    }
    else if eq ch '/' {
      if eq '/' load8 l.index {
        while and neq load8 l.index 10 neq load8 l.index 0 {
          = @l.index + 1 l.index;
          = @l.column + 1 l.column;
          = @token.column l.column;
        }
      }
      else {
        = @token.type T_DIV;
        = @done 1;
      }
    }
    else if eq ch '"' {
      let delimiter ch;
      let loop_done 0;
      while eq loop_done 0 {
        if eq load8 l.index 0 {
          lexer_error("unfinished string\n");
          = @token.type T_EOF;
          = @loop_done 1;
          = @done 1;
        }
        else if eq load8 l.index 92 { // `\\`
          if eq load8 + l.index 1 '0' {
            store8 l.index 0;
            = @l.index + 1 l.index;
          }
          if eq load8 + l.index 1 'n' {
            store8 l.index 10;
            = @l.index + 1 l.index;
          }
          = @l.index + 1 l.index;
          = @l.column + 1 l.column;
        }
        else if eq load8 l.index delimiter {
          = @loop_done 1;
        }
        else {
          = @l.index + 1 l.index;
          = @l.column + 1 l.column;
        }
      }
      = @token.buffer + 1 token.buffer;
      = @l.column + 1 l.column;
      = @token.type T_CSTRING;
      = @token.length - l.index token.buffer;
      = @l.index + 1 l.index;
      = @done 1;
    }
    else if eq ch '=' {
      = @token.type T_ASSIGN;
      = @done 1;
    }
    else if eq ch ',' {
      = @token.type T_COMMA;
      = @done 1;
    }
    else if eq ch '@' {
      = @token.type T_AT;
      = @done 1;
    }
    else if eq ch '#' {
      = @token.type T_DEREF;
      = @done 1;
    }
    else if eq ch '+' {
      = @token.type T_ADD;
      = @done 1;
    }
    else if eq ch '-' {
      if eq load8 l.index '>' {
        = @l.index + 1 l.index;
        = @l.column + 1 l.column;
        = @token.length + 1 token.length;
        = @token.type T_ARROW;
        = @done 1;
      }
      else {
        = @token.type T_SUB;
        = @done 1;
      }
    }
    else if eq ch '*' {
      = @token.type T_MUL;
      = @done 1;
    }
    else if eq ch '%' {
      = @token.type T_DIVMOD;
      = @done 1;
    }
    else if eq ch '<' {
      = @token.type T_LT;
      = @done 1;
    }
    else if eq ch '>' {
      = @token.type T_GT;
      = @done 1;
    }
    else if eq ch ';' {
      = @token.type T_SEMICOLON;
      = @done 1;
    }
    else if eq ch '(' {
      = @token.type T_LEFT_P;
      = @done 1;
    }
    else if eq ch ')' {
      = @token.type T_RIGHT_P;
      = @done 1;
    }
    else if eq ch '[' {
      = @token.type T_LEFT_BRACKET;
      = @done 1;
    }
    else if eq ch ']' {
      = @token.type T_RIGHT_BRACKET;
      = @done 1;
    }
    else if eq ch '{' {
      = @token.type T_LEFT_CURLY;
      = @done 1;
    }
    else if eq ch '}' {
      = @token.type T_RIGHT_CURLY;
      = @done 1;
    }
    else if or or or eq ch ' ' eq ch 9 eq ch 11 eq ch 12 { // ` `, `\t`, `\v`, `\f`
    }
    else if eq ch 0 {
      = @token.type T_EOF;
      = @done 1;
    }
    else if eq ch 39 { // `\'`
      = @ch load8 l.index;
      = @token.buffer l.index;
      = @l.index + 1 l.index;
      = @l.column + 1 l.column;
      if neq load8 l.index 39 {
        = @token.type T_EOF;
        lexer_error("missing closing `'`\n");
      }
      else {
        = @l.index + 1 l.index;
        = @l.column + 1 l.column;
        = @token.v ch;
        = @token.type T_NUMBER;
        = @token.length 1;
      }
      = @done 1;
    }
    else {
      if or or is_alpha(ch) is_extended_ascii(ch) eq ch '_' {
        lexer_read_symbol();
        = @done 1;
      }
      else if is_digit(ch) {
        lexer_read_number();
        let number str_to_u64(token.buffer, token.length);
        = @token.v number;
        = @done 1;
      }
      else {
        let it "";
        = @it tmp_it;
        tmp_push_cstr("unrecognized token `");
        tmp_push_str(token.buffer, token.length);
        tmp_push_cstr("`\n");
        lexer_error(it);
        = @tmp_it it;
        = @token.type T_EOF;
        = @done 1;
      }
    }
  }
  = @token.line l.line;
}

fn lexer_init(any filename, any source) -> none {
  // initialize lexer state
  = @l.filename filename;
  = @l.source source;
  = @l.index source;
  = @l.line 1;
  = @l.column 1;
  = @l.status NoError;

  // initialize lexer token
  = @token.buffer source;
  = @token.length 0;
  = @token.type T_EOF;
  = @token.v 0;
  = @token.filename filename;
  = @token.source source;
  = @token.line 1;
  = @token.column 1;

  let t (
    "T_EOF",

    "T_IDENTIFIER",
    "T_NUMBER",
    "T_CSTRING",
    "T_ASSIGN",
    "T_COMMA",
    "T_AT",
    "T_DEREF",
    "T_ADD",
    "T_SUB",
    "T_MUL",
    "T_DIV",
    "T_DIVMOD",
    "T_LSHIFT",
    "T_RSHIFT",
    "T_LT",
    "T_GT",
    "T_AND",
    "T_LOGICAL_NOT",
    "T_OR",
    "T_EQ",
    "T_NEQ",
    "T_SEMICOLON",
    "T_POP",
    "T_CONST",
    "T_LET",
    "T_MEMORY",
    "T_PRINT",
    "T_INCLUDE",
    "T_FN",
    "T_ARROW",
    "T_WHILE",
    "T_IF",
    "T_ELSE",
    "T_LEFT_P",
    "T_RIGHT_P",
    "T_LEFT_BRACKET",
    "T_RIGHT_BRACKET",
    "T_LEFT_CURLY",
    "T_RIGHT_CURLY",
    "T_STORE64",
    "T_STORE32",
    "T_STORE16",
    "T_STORE8",
    "T_LOAD64",
    "T_LOAD32",
    "T_LOAD16",
    "T_LOAD8",
    "T_SIZEOF",

    "T_NONE",
    "T_ANY",
    "T_UNSIGNED64",
    "T_CSTR"
  );
  = @token_type_str @t;
  assert(eq (* sizeof any MAX_TOKEN_TYPE) (sizeof t), "mismatch between token_type_str and MAX_TOKEN_TYPE\n");
}
