// lex.spl

// enum Token_type {
const T_EOF 0;

const T_IDENTIFIER 1;
const T_NUMBER 2;
const T_CSTRING 3;
const T_ASSIGN 4;
const T_COMMA 5;
const T_AT 6;
const T_DEREF 7;
const T_ADD 8;
const T_SUB 9;
const T_MUL 10;
const T_DIV 11;
const T_DIVMOD 12;
const T_LSHIFT 13;
const T_RSHIFT 14;
const T_LT 15;
const T_GT 16;
const T_AND 17;
const T_LOGICAL_NOT 18;
const T_OR 19;
const T_EQ 20;
const T_NEQ 21;
const T_SEMICOLON 22;
const T_POP 23;
const T_CONST 24;
const T_LET 25;
const T_MEMORY 26;
const T_PRINT 27;
const T_INCLUDE 28;
const T_FN 29;
const T_ARROW 30;
const T_WHILE 31;
const T_IF 32;
const T_ELSE 33;
const T_LEFT_P 34;
const T_RIGHT_P 35;
const T_LEFT_BRACKET 36;
const T_RIGHT_BRACKET 37;
const T_LEFT_CURLY 38;
const T_RIGHT_CURLY 39;
const T_STORE64 40;
const T_STORE32 41;
const T_STORE16 42;
const T_STORE8 43;
const T_LOAD64 44;
const T_LOAD32 45;
const T_LOAD16 46;
const T_LOAD8 47;
const T_SIZEOF 48;

// built-in types
const T_NONE 49;
const T_ANY 50;
const T_UNSIGNED64 51;
const T_CSTR 52;

const MAX_TOKEN_TYPE 53;
// };

let token_type_str "";

let token.buffer 0;
let token.length 0;
let token.type 0;
let token.v 0;
let token.filename "";
let token.source "";
let token.line 0;
let token.column 0;
let Token.size +++++++
  sizeof token.buffer
  sizeof token.length
  sizeof token.type
  sizeof token.v
  sizeof token.filename
  sizeof token.source
  sizeof token.line
  sizeof token.column;

// struct Lexer {
let l.filename "";
let l.source "";
let l.index 0;
let l.line 0;
let l.column 0;
let l.status 0;
// };

fn token_init(any token, any buffer, u64 length, u64 type, any filename, any source) -> none {
  = token buffer;   = @token + sizeof token.buffer token;
  = token length;   = @token + sizeof token.length token;
  = token type;     = @token + sizeof token.type token;
  = token 0;        = @token + sizeof token.v token;
  = token filename; = @token + sizeof token.filename token;
  = token source;   = @token + sizeof token.source token;
  = token 1;        = @token + sizeof token.line token;
  = token 1;        = @token + sizeof token.column token;
}

fn token_print(any token) -> none {
  let tmp tmp_it;
  let p tmp_push_cstr;
  // oh boy, i really REALLY need proper structs
  let buffer #token;   = @token + sizeof token.buffer token;
  let length #token;   = @token + sizeof token.length token;
  let type #token;     = @token + sizeof token.type token;
  let value #token;    = @token + sizeof token.v token;
  let filename #token; = @token + sizeof token.filename token;
  let source #token;   = @token + sizeof token.source token;
  let line #token;     = @token + sizeof token.line token;
  let column #token;   = @token + sizeof token.column token;

  if eq filename 0 = @filename "none";

  p("buffer   = "); tmp_push_str(buffer, length);
  p("\n");
  p("length   = "); tmp_push_u64(length);
  p("\n");
  p("type     = "); tmp_push_u64(type);
  p("\n");
  p("filename = "); p(filename);
  p("\n");
  p("line     = "); tmp_push_u64(line);
  p("\n");
  p("column   = "); tmp_push_u64(column);
  p("\n");

  puts(1, tmp);
  = @tmp_it tmp;
}

fn is_digit(u64 ch) -> u64 {
  and > ch - '0' 1 < ch + '9' 1;
}

fn is_hex(u64 ch) -> u64 {
  or or is_digit(ch) (and > ch - 'a' 1 < ch + 'f' 1) (and > ch - 'A' 1 < ch + 'F' 1);
}

fn is_alpha(u64 ch) -> u64 {
  or (and > ch - 'a' 1 < ch + 'z' 1) (and > ch - 'A' 1 < ch + 'Z' 1);
}

fn is_extended_ascii(u64 ch) -> u64 {
  and > ch 127 < ch 255;
}

fn to_lower(u64 ch) -> u64 {
  if and > ch - 'A' 1 < ch + 'Z' 1 {
    = @ch + 32 ch;
  }
  ch;
}

fn lexer_error(cstr message) -> none {
  if eq l.status NoError {
    puts(STDERR_FILENO, "[lex-error]: ");
    puts(STDERR_FILENO, message);
    = @l.status Error;
  }
}

fn lexer_init(any filename, any source) -> none {
  // initialize lexer state
  = @l.filename filename;
  = @l.source source;
  = @l.index source;
  = @l.column 1;
  = @l.status NoError;

  // initialize lexer token
  = @token.buffer source;
  = @token.length 0;
  = @token.type T_EOF;
  = @token.v 0;
  = @token.filename filename;
  = @token.source source;
  = @token.line 1;
  = @token.column 1;

  let t (
    "T_EOF",

    "T_IDENTIFIER",
    "T_NUMBER",
    "T_CSTRING",
    "T_ASSIGN",
    "T_COMMA",
    "T_AT",
    "T_DEREF",
    "T_ADD",
    "T_SUB",
    "T_MUL",
    "T_DIV",
    "T_DIVMOD",
    "T_LSHIFT",
    "T_RSHIFT",
    "T_LT",
    "T_GT",
    "T_AND",
    "T_LOGICAL_NOT",
    "T_OR",
    "T_EQ",
    "T_NEQ",
    "T_SEMICOLON",
    "T_POP",
    "T_CONST",
    "T_LET",
    "T_MEMORY",
    "T_PRINT",
    "T_INCLUDE",
    "T_FN",
    "T_ARROW",
    "T_WHILE",
    "T_IF",
    "T_ELSE",
    "T_LEFT_P",
    "T_RIGHT_P",
    "T_LEFT_BRACKET",
    "T_RIGHT_BRACKET",
    "T_LEFT_CURLY",
    "T_RIGHT_CURLY",
    "T_STORE64",
    "T_STORE32",
    "T_STORE16",
    "T_STORE8",
    "T_LOAD64",
    "T_LOAD32",
    "T_LOAD16",
    "T_LOAD8",
    "T_SIZEOF",

    "T_NONE",
    "T_ANY",
    "T_UNSIGNED64",
    "T_CSTR"
  );
  = @token_type_str @t;
  assert(eq (* sizeof any MAX_TOKEN_TYPE) (sizeof t), "mismatch between token_type_str and MAX_TOKEN_TYPE\n");
}

fn next -> none {
  = @token.buffer l.index;
  = @token.length 1;
  = @token.line l.line;
  = @token.column l.column;
}

fn lexer_next -> none {
  let should_exit 0;
  let ch 0;
  while not should_exit {
    next();
    = @ch load8 token.buffer;
    = @token.filename l.filename;
    = @token.source l.source;
    = @token.column l.column;
    = @l.index + 1 l.index;
    = @l.column + 1 l.column;

    if eq ch 10 {
      = @l.line + 1 l.line;
      = @l.column 1;
    }
    else if eq ch '=' {
      = @token.type T_ASSIGN;
      = @should_exit 1;
    }
    else if eq ch ',' {
      = @token.type T_COMMA;
      = @should_exit 1;
    }
    else if eq ch '@' {
      = @token.type T_AT;
      = @should_exit 1;
    }
    else if eq ch '#' {
      = @token.type T_DEREF;
      = @should_exit 1;
    }
    else if eq ch '+' {
      = @token.type T_ADD;
      = @should_exit 1;
    }
    else if eq ch '-' {
      = @token.type T_SUB;
      = @should_exit 1;
    }
    else if eq ch '*' {
      = @token.type T_MUL;
      = @should_exit 1;
    }
    else if eq ch '%' {
      = @token.type T_DIVMOD;
      = @should_exit 1;
    }
    else if eq ch '<' {
      = @token.type T_LT;
      = @should_exit 1;
    }
    else if eq ch '>' {
      = @token.type T_GT;
      = @should_exit 1;
    }
    else if eq ch ';' {
      = @token.type T_SEMICOLON;
      = @should_exit 1;
    }
    else if eq ch '(' {
      = @token.type T_LEFT_P;
      = @should_exit 1;
    }
    else if eq ch ')' {
      = @token.type T_RIGHT_P;
      = @should_exit 1;
    }
    else if eq ch '[' {
      = @token.type T_LEFT_BRACKET;
      = @should_exit 1;
    }
    else if eq ch ']' {
      = @token.type T_RIGHT_BRACKET;
      = @should_exit 1;
    }
    else if eq ch '{' {
      = @token.type T_LEFT_CURLY;
      = @should_exit 1;
    }
    else if eq ch '}' {
      = @token.type T_RIGHT_CURLY;
      = @should_exit 1;
    }
    else if or or or eq ch ' ' eq ch 9 eq ch 11 eq ch 12 { // ` `, `\t`, `\v`, `\f`
    }
    else if eq ch 0 {
      = @token.type T_EOF;
      = @should_exit 1;
    }
    else if eq ch 39 { // `\'`
      = @ch load8 l.index;
      = @token.buffer l.index;
      = @l.index + 1 l.index;
      = @l.column + 1 l.column;
      if neq load8 l.index 39 {
        = @token.type T_EOF;
        lexer_error("missing closing `'`\n");
      }
      else {
        = @l.index + 1 l.index;
        = @l.column + 1 l.column;
        = @token.v ch;
        = @token.type T_NUMBER;
        = @token.length 1;
      }
      = @should_exit 1;
    }
    else {
      if or or is_alpha(ch) is_extended_ascii(ch) eq ch '_' {
        = @should_exit 1;
      }
      else if is_digit(ch) {
        = @should_exit 1;
      }
      else {
        let it "";
        = @it tmp_it;
        tmp_push_cstr("unrecognized token `");
        tmp_push_str(token.buffer, token.length);
        tmp_push_cstr("`\n");
        lexer_error(it);
        = @tmp_it it;
        = @token.type T_EOF;
        = @should_exit 1;
      }
    }
  }
  = @token.line l.line;
}
